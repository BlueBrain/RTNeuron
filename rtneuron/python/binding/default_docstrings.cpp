// File automatically generated by
// /home/hernando/dev/src/RTNeuron/Pydoxine/CMake/../docstrings/sphinx_ext/cpp_docstrings.py

// clang-format off
void initDocStrings(std::map<std::string, std::string> &m)
{
m["bbp::rtneuron::AttributeMap"] = "\nAn key-value storage with additional capabilities in the native code side.\n\nAn attribute map is a table container that stores key-value pairs where the keys are strings and the values are scalars of type bool, int, float, string, wrapped enums and AttributeMap or Python lists of scalars of type bool, int, float, string and wrapped enums.\n\nOther wrapped types can be used as values only if their documentation states so.\n\n\nThe attribute keys are presented as regular attributes in Python. This class defines special '__setattr__' and '__getattr__' methods to handle attribute read/writes and translation of types to/from the native code. Writting to a non existing attribute creates it. Accessing a non existing attribute raises a ValueError exception. Trying to set an attribute with an unsupported type raises a KeyError exception.**Note**\n    An AttributeMap cannot be nested as part of a list of values.\n\n\nExamples: ::\n\n    a = AttributeMap()               # Create a new attribute map.\n    a.x = 10.0                       # Set a new attribute.\n    print a.x + 3.3                  # Retrieve the attribute value.\n    a.x = [1, \"hi\", False]           # Resetting the previous attribute.\n    a.nested = AttributeMap()        # Nesting an attribute map.\n    a.nested.x = [1, AttributeMap()] # raises, AttributeMap cannot be in a list.\n    a.nested.x = dict                # raises, invalid type in assignment.\n    a.nested.colors = ColorMap()     # OK if ColorMap has been made available\n                                     # to AttributeMap in the wrapping.\n    a.colors = [ColorMap(), 1, \"a\"]  # If the above works, this will also do.\n\n\nNative code objects that hold attribute maps can provide extra handles for attribute modification. This implies that trying to set attribute names/values unsupported by a holder can also raise exceptions.\n\nTab completion inside an IPython shell works by the internal redefinition of the '__dir__' method. The string conversion operator is also defined to print the attributes and their values. \n";
m["bbp::rtneuron::AttributeMap::attributeChanged"] = "\nSignal emitted when an attribute has been changed.\n\nThe name of the changed attribute is passed as the signal parameter. \n";
m["bbp::rtneuron::Camera"] = "\nA camera represents the frustum and the model part of the model-view transformation used for a single view.\n\nThe view part of the transformation will be handled internally by the Equalizer backend as this is required in cluster configurations. \n";
m["bbp::rtneuron::Camera::getProjectionFrustum"] = "\nGets the frustum definition of a perspective projection.\n\nThe near parameters returned is just meant to indicate the field of view the actual parameters used for rendering are adjusted to the scene being displayed.\n\nThe results are undefined if the camera is set as orthographic.\n\n**Return**\n    A tuple with left, right, top, bottom, near\n\n**Version**\n    2.7 \n\n";
m["bbp::rtneuron::Camera::getProjectionMatrix"] = "\nGet the camera projection matrix.\n\n**Return**\n    An OpenGL ready 4x4 numpy matrix\n\n**Version**\n    2.12 \n\n";
m["bbp::rtneuron::Camera::getProjectionOrtho"] = "\nGets the camera frustum for orthographic projection.\n\nThe results are undefined if the camera is using perspective projection.\n\n**Return**\n    left, right, bottom and top\n\n**Version**\n    2.4 \n\n";
m["bbp::rtneuron::Camera::getProjectionPerspective"] = "\nGets the parameters of a perspective projection.\n\nThe results are undefined if the camera is set as orthographic.\n\n**Return**\n    A tuple with the vertical field of view and the aspect ratio.\n\n**Version**\n    2.4 For the Python wrapping. \n\n";
m["bbp::rtneuron::Camera::getView"] = "\nGet the camera position.\n\n**Return**\n    A tuple (position, (axis, angle)) where position and axis are [x, y, z] lists and the angle is in degrees. \n\n";
m["bbp::rtneuron::Camera::getViewMatrix"] = "\nGet the camera modelview matrix.\n\n**Return**\n    An OpenGL ready 4x4 numpy matrix\n\n**Version**\n    2.12 \n\n";
m["bbp::rtneuron::Camera::isOrtho"] = "\n**Return**\n    True if the camera is applying orthographic projection, false otherwise \n\n";
m["bbp::rtneuron::Camera::makeOrtho"] = "\nSets the camera to do orthographic projection preserving the current frustum. \n";
m["bbp::rtneuron::Camera::makePerspective"] = "\nSets the camera to do perspective projection preserving the current frustum. \n";
m["bbp::rtneuron::Camera::projectPoint"] = "\nReturns the 2D projected coordinates of a 3D point in world coordinates.\n\nThe third coordinate just represents if the point is in front of (+1), coincident (0) or behind (-1) the camera. If any coordinate is <-1 or >1 that means that the point is outside the frustum. \n";
m["bbp::rtneuron::Camera::setProjectionFrustum"] = "\nSets the camera frustum for perspective projection.\n\nNear and far are autoadjusted by the renderer. The near value provided here is used to infer the field of view. No auto aspect ratio conservation is performed.\n\n**Version**\n    2.7 \n\n";
m["bbp::rtneuron::Camera::setProjectionOrtho"] = "\nSets the camera frustum for orthographic projections.\n\n**Version**\n    2.7 \n\n";
m["bbp::rtneuron::Camera::setProjectionPerspective"] = "\nChange the vertical field of view of the perspective projection.\n\nThe aspect ratio is inferred from the current projection matrix.\n\n**Parameters**\n\n     ``verticalFOV`` -       Angle in degrees. \n\n";
m["bbp::rtneuron::Camera::setView"] = "\nSets the camera position\n\n**Parameters**\n\n     ``position`` -       The world position of the camera \n\n     ``orientation`` -       A tuple ((x, y, z), angle) with a rotation to be applied to the camera (the initial view direction is looking down the negative z axis). The angle is in degrees. \n\n";
m["bbp::rtneuron::Camera::setViewLookAt"] = "\nThe same as gluLookAt.\n\nThis method also sets the home position and pivotal point for manipulators that take it into account. \n";
m["bbp::rtneuron::Camera::unprojectPoint"] = "\nReturn the 3D world coordinates of a projected point.\n\n**Version**\n    2.12 \n\n**Parameters**\n\n     ``point`` -       The 2D normalized device coordinates of the point \n\n     ``z`` -       The z value of the 2D point in camera coordinates. Note that for points in front of the camera this value is negative.\n\n";
m["bbp::rtneuron::Camera::viewDirty"] = "\nEmitted whenever the modelview matrix is modified by the rendering engine. \n\n**Version**\n    2.6 \n\n";
m["bbp::rtneuron::CameraManipulator"] = "\nBase class for all camera manipulators. \n\nInherits from noncopyable\n\nSubclassed by bbp.rtneuron.CameraPathManipulator, bbp.rtneuron.TrackballManipulator, bbp.rtneuron.VRPNManipulator\n";
m["bbp::rtneuron::CameraPath"] = "\nA sequence of camera keyframes with timestamps. \n";
m["bbp::rtneuron::CameraPath::KeyFrame"] = "\nPosition, orientation and stereo correction of a given timestamp. \n";
m["bbp::rtneuron::CameraPath::KeyFrame::getStereoCorrection"] = "\nA scalar multiplicative factor for the interocular distance. \n";
m["bbp::rtneuron::CameraPath::addKeyFrame(double, View&)"] = "\nAdds a new key frame to the path from the camera and stereo correction of the given view. \n\nIf there's a key frame with that exact timing, it is replaced. Changing the old key frame from an existing reference does not affect the camera path. \n";
m["bbp::rtneuron::CameraPath::addKeyFrame(double, const KeyFramePtr&)"] = "\nAdds a new key frame to the path. \n\nIf there's a key frame with that exact timing, it is replaced. Changing the old key frame from an existing reference does not affect the camera path. \n";
m["bbp::rtneuron::CameraPath::clear"] = "\nClears the camera path. \n";
m["bbp::rtneuron::CameraPath::getKeyFrame"] = "\nReturns the key frame at a given position. \n\nThrows if the index is out of bounds. \n";
m["bbp::rtneuron::CameraPath::getKeyFrames"] = "\nReturn a list of tuples (times, KeyFrame).\n\nIf key frames are modified the camera path will be updated. \n";
m["bbp::rtneuron::CameraPath::getStartTime"] = "\nThe time of the earliest key frame of NaN if the path is empty. \n";
m["bbp::rtneuron::CameraPath::getStopTime"] = "\nThe time of the latest key frame of NaN if the path is empty. \n";
m["bbp::rtneuron::CameraPath::load"] = "\nLoads a camera path from the given file. \n";
m["bbp::rtneuron::CameraPath::removeKeyFrame"] = "\nRemoves the keyframe at the given position. \n\nThrows if the index is out of bounds. \n";
m["bbp::rtneuron::CameraPath::replaceKeyFrame"] = "\nReplaces a keyframe at a given position with a new one. \n\nThrows if the index is out of bounds. \n";
m["bbp::rtneuron::CameraPath::save"] = "\nWrites this camera path to the given file. \n";
m["bbp::rtneuron::CameraPath::setKeyFrames"] = "\nReplaces the current path by a new one. \n\n**Parameters**\n\n     ``frames`` -       A dictionary with time in seconds as keys and KeyFrames as values \n\n";
m["bbp::rtneuron::CameraPathManipulator::LoopMode"] = "\nThe loop mode defines what to do when the end of the camera path is reached. \n\n*Values:*\n * LOOP_NONE: \nDo nothing. \n * LOOP_REPEAT: \nStart over the camera path. \n * LOOP_SWING: \nPlay the camera path in reserve until the start is reached and repeat \n";
m["bbp::rtneuron::CameraPathManipulator::getFrameDelta"] = "\nReturn the frame delta in milliseconds \n";
m["bbp::rtneuron::CameraPathManipulator::getKeyFrame"] = "\nGet a interpolated keyframe at the given timestamp\n\n**Return**\n    A tuple (position, (axis, angle), stereoCorection) where position and axis are [x, y, z] lists and the angle is in degrees. \n\n**Version**\n    2.4 \n\n";
m["bbp::rtneuron::CameraPathManipulator::getLoopMode"] = "";
m["bbp::rtneuron::CameraPathManipulator::getPlaybackStart"] = "";
m["bbp::rtneuron::CameraPathManipulator::getPlaybackStop"] = "";
m["bbp::rtneuron::CameraPathManipulator::load"] = "\nLoads a camera path from a file.\n\nIf the camera path contains a single keyframe the loopmode is automatically set to LOOP_NONE.\n\nThrows if an error occurs reading the file. \n";
m["bbp::rtneuron::CameraPathManipulator::setFrameDelta"] = "\nSets the delta time between keyframe samples (in milliseconds)\n\nUse a positive value to set a fixed delta between rendered frames. A value equal to 0 means that the camera path has to be played back in real-time. \n";
m["bbp::rtneuron::CameraPathManipulator::setLoopMode"] = "";
m["bbp::rtneuron::CameraPathManipulator::setPlaybackInterval"] = "\nOverrides the start and stop time of the camera path.\n\n**Parameters**\n\n     ``start`` -       Milliseconds \n\n     ``end`` -       Milliseconds \n\n";
m["bbp::rtneuron::CameraPathManipulator::setPlaybackStart"] = "\nOverrides the start time of the camera path. \n\n**Parameters**\n\n     ``start`` -       Milliseconds \n\n";
m["bbp::rtneuron::CameraPathManipulator::setPlaybackStop"] = "\nOverrides the stop time of the camera path\n\n**Parameters**\n\n     ``end`` -       Milliseconds \n\n";
m["bbp::rtneuron::ColorMap::getColor"] = "\nReturns the color for the given value.\n\n**Return**\n    The color at the given value using linear interpolation of the control points. \n\n**Parameters**\n\n     ``value`` -       Clamped to current range before sampling the internal texture. \n\n";
m["bbp::rtneuron::ColorMap::getPoints"] = "\nThe control points of this color map. \n";
m["bbp::rtneuron::ColorMap::getRange"] = "\nReturn the color map range. \n";
m["bbp::rtneuron::ColorMap::load"] = "\nLoad a color map from the file with the given name.\n\nThrows if an error occurs. \n";
m["bbp::rtneuron::ColorMap::save"] = "\nSave a color map to a file with the given name.\n\nThrows if an error occurs. \n";
m["bbp::rtneuron::ColorMap::setPoints"] = "\nCreates the internal look up table using the map of (value, color) points given.\n\n**Parameters**\n\n     ``colorPoints`` -       The control points dictionary. The keys must be floats and the items are 4-float tuples (RGBA). If any channel is outside the range [0, 1] the underlying color map will be undefined. \n\n";
m["bbp::rtneuron::ColorMap::setRange"] = "\nChanges the colormap range adjusting the point values.\n\nThe value of the points are ajusted to the new range and the dirty signal is emitted. \n";
m["bbp::rtneuron::ColorMap::setTextureSize"] = "\nSet the resolution of the internal texture used for the colormap (measured in texels).\n\nThe minimum texture size is bounded to 2 texels. \n";
m["bbp::rtneuron::ColorScheme"] = "\nColoring mode for structural rendering of neurons. \n\n*Values:*\n * SOLID: \nRender the whole neuron with its primary color. \n * RANDOM: \nUse a random color for the whole neuron. \n * BY_BRANCH_TYPE: \nRender dendrites with the primary color and axons with the secondary color. \n * BY_WIDTH: \nApply a different color to each vertex based on it branch width. The color is interpolated from a color map computed using both the primary and secondary colors.\n\nIf simulation display is enabled, the alpha channel of the colormap is used to modulate the final rendering color. \n * BY_DISTANCE_TO_SOMA: \nApply per-vertex colors based on the distance to the soma. The colormap used is derived from the primary and secondary colors by default, unless a ``by_distance_to_soma`` colormap is set in the ``colormaps`` attribute or the neuron object.\n\nIf simulation display is enabled, the alpha channel of the colormap is used to modulate the final rendering color. \n * NUM_COLOR_SCHEMES";
m["bbp::rtneuron::DataBasePartitioning"] = "\nPartitioning scheme to be applied to neurons in DB (sort-last) rendering configurations. \n\n*Values:*\n * NONE * ROUND_ROBIN * SPATIAL";
m["bbp::rtneuron::NeuronLOD"] = "\nModels used for level of detail representation of neurons. \n\n*Values:*\n * MEMBRANE_MESH * TUBELETS * HIGH_DETAIL_CYLINDERS * LOW_DETAIL_CYLINDERS * DETAILED_SOMA * SPHERICAL_SOMA * NUM_NEURON_LODS";
m["bbp::rtneuron::RTNeuron"] = "\nThe main application class. \n\nThis class manages the Equalizer configuration and is the factory for other classes that are tied to a configuration (e.g. the scenes). \n\nInherits from std.enable_shared_from_this< RTNeuron >\n";
m["bbp::rtneuron::RTNeuron::RTNeuron(list, const AttributeMap&)"] = "\n**Version**\n    2.4 required for the *profile* attribute. \n\n**Parameters**\n\n     ``argv`` -       The command line argument list. \n\n     ``attributes`` -       Global application options, including:\n       *afferent_syn_color* (floatx3[+1]): Default color to use for afferent synapse glyphs\n\n       *autoadjust_simulation_window* (bool) Whether the simulation player window should be adjusted automatically. Simulation window adjustment occurs when:\n         ::`SimulationPlayer.setTimestamp` is called\n\n         ::`SimulationPlayer.play` is called\n\n         A new simulation timestamp has been mapped and is ready for displaying. Auto-adjustment will not try to obtain the latest simulation data if it can lead to a deadlock (e.g. when the engine is already trying to do it for a previously requested timestamp).\n\n\n       *efferent_syn_color* (floatx3[+1]): Default color to use for efferent synapse glyphs\n\n       *has_gui* (bool): True to indicate the RTNeuron object that it's running inside a QT application.\n\n       *neuron_color* (floatx3[+1]): Default color for neurons\n\n       *soma_radii* (AttributeMap): An attribute map indexed with morphology type names as attribute names and radii as attribute values.\n\n       *soma_radius* (float): Default soma radius to use if no additional information is available.\n\n       *profile* (AttributeMap): An attribute map with options for profiling:\n         enable (bool): Enable frame time profiling\n\n         logfile (string): Log file name to write frame times.\n\n         compositing (bool): False to disable frame compositing, True or non present otherwise.\n\n\n       *view* (AttributeMap): An attribute map with the default view parameters (e.g. background, lod_bias, ...).\n\n       *window_width* (int): Width of the application window in pixels.\n\n       *window_height* (int): Height of the application window in pixels.\n\n\n";
m["bbp::rtneuron::RTNeuron::createConfig"] = "\nDeprecated, use init instead.\n";
m["bbp::rtneuron::RTNeuron::createScene"] = "\nCreates a Scene to be used in this application. \n\nThe attribute map includes scene attributes that are passed to the scene constructor. See here for details.\n\nCurrently, all scenes to be used inside a config must be created in all nodes before init is called.\n\nThe application does not hold any reference to the returned scene. If the caller gets rid of the returned reference and no view holds the scene, the scene will be deallocated.\n\nDo not call from outside the main thread.\n\n**See**\n    Scene.Scene \n\n";
m["bbp::rtneuron::RTNeuron::eventProcessorUpdated"] = "";
m["bbp::rtneuron::RTNeuron::exit"] = "\nStops all rendering and cleans up all the resource from the current Equalizer configuration. \n\nThe exited signal will be emitted when the config is considered to be done. Rendering is not stopped yet at that point.\n\nDo not call from outside the main thread. **Version**\n    2.4 \n\n";
m["bbp::rtneuron::RTNeuron::exitConfig"] = "\nDeprecated, use exit instead.  \n";
m["bbp::rtneuron::RTNeuron::exited"] = "\nEmitted while the config is done in Config.setDone.\n\nFrame rendering is not finished at the point. The signal indicates that the lifetime of objects that might be referenced outside of the library (e.g. in python) is about to end, so object destructions can be scheduled accordingly.\n\n**Version**\n    2.7 \n\n";
m["bbp::rtneuron::RTNeuron::frame"] = "\nTrigger a frame. \n\nIf the rendering is paused, triggers the rendering of exactly one frame.\n\nIf the rendering loop is running, triggers a redraw request if the rendering loop was waiting for this event.\n\nDo not call from outside the main thread in the application node. \n";
m["bbp::rtneuron::RTNeuron::frameIssued"] = "\nEmitted after the internal rendering loop has finished issuing a frame. \n\nThe frame is not necessarily finished at this point, but all the distributed objects are guaranteed to have been committed. This can be used for animations.\n\n**Version**\n    2.2 \n\n";
m["bbp::rtneuron::RTNeuron::getActiveViewEventProcessor"] = "";
m["bbp::rtneuron::RTNeuron::getAllViews"] = "\nReturns the vector of active or inactive views which belong to any layout. \n";
m["bbp::rtneuron::RTNeuron::getAttributes"] = "\nReturns a modifiable attribute map. \n\nThese attributes can be modified at runtime. \n";
m["bbp::rtneuron::RTNeuron::getPlayer"] = "\nReturns the interface object to simulation playback. \n\nA single player by default. May become an external object and shared by different views. \n";
m["bbp::rtneuron::RTNeuron::getVersionString"] = "\nReturns the version string to print out. \n";
m["bbp::rtneuron::RTNeuron::getViews"] = "\nReturns the vector of active views. \n";
m["bbp::rtneuron::RTNeuron::idle"] = "\nEmitted when the application is idle, e.g. waiting for (user)events.\n\n**Version**\n    2.7 \n\n";
m["bbp::rtneuron::RTNeuron::init"] = "\nCreates the view (windows, rendering threads, event processing...) Throws if there's a view already created. \n\nIn parallel rendering configurations this function needs to call eq.client.initLocal to launch the rendering clients. Instead of blocking forever inside this function, a new thread will be created for the client loop.\n\nThis function blocks until the application (or rendering client) loop is guaranteed to have been started. The rendering loop starts paused ::`RTNeuron.resume` must be called to start the rendering.\n\nDo not call from outside the main thread.\n\n**Version**\n    2.4 \n\n**Parameters**\n\n     ``config`` -       Path to an Equalizer configuration file or hwsd session name. \n\n";
m["bbp::rtneuron::RTNeuron::pause"] = "\nBlock rendering loop after the current frame finishes. \n\nDo not call from outside the main thread in the application node. \n";
m["bbp::rtneuron::RTNeuron::record"] = "\nHigh level function to dump the rendered frames to files. \n\nWhen called, the rendering loop is resumed if paused.\n\nIf a camera path is given, a new camera path manipulator is created and assigned to all active views. Any previous camera manipulator will be overriden.\n\nRecording is stopped automatically when: The camera path end is reached.\n\n The end of the simulation window is reached.\n\n The maximum number of frames to render is reached. whichever occurs first.\n\n\nThis function does not wait until all frames are finished (you can set a callback to frameIssued to count frames or use waitRecord). If the simulation delta is set to 0, no simulation playback will be performed (in that case, the current simulation window remains unmodified).\n\nIf the simulation window is invalid (start >= stop) it will not be considered to stop the recording.\n\nIdle anti-alias appears disabled when using this function.\n\nChanging simulation playback parameters in the player API will interfere with the results of this function.\n\nDo not call from outside the main thread.\n\n**Version**\n    2.3 \n\n";
m["bbp::rtneuron::RTNeuron::resume"] = "\nResume the rendering loop. \n\nDo not call from outside the main thread in the application node. \n";
m["bbp::rtneuron::RTNeuron::setShareContext"] = "";
m["bbp::rtneuron::RTNeuron::textureUpdated"] = "\nEmitted after the texture that captures the appNode rendering was updated.\n\nThis signal is tied to the existence of the GUI widget which takes this notification to render the UI and the texture.\n\n**Version**\n    2.7 \n\n";
m["bbp::rtneuron::RTNeuron::useLayout"] = "\nChange the Equalizer layout. \n\nThrows if the layout does not exist or if there is no configuration initialized. Do not call outside the main thread. **Version**\n    2.4 \n\n";
m["bbp::rtneuron::RTNeuron::wait"] = "\nWait for the Equalizer application loop to exit. \n\nThis function returns inmediately if not config is active. Otherwise it blocks until some event makes the application loop to exit.\n\nWhile a thread is blocked in this function, calls to init. or waitRecord will block. \n";
m["bbp::rtneuron::RTNeuron::waitFrame"] = "\nWait for a new frame to be finished. \n\nReturns inmediately if no config is active. Exiting the config will also unlock the caller. Do not call from outside the main thread in the application node. \n";
m["bbp::rtneuron::RTNeuron::waitFrames"] = "\nWait for at least n frames to be finished. \n\nThis function resumes the rendering loop. Returns inmediately if no config is active. Exiting the config will also unlock the caller. More frames may be generated before the function returns. Do not call from outside the main thread in the application node.\n\n**Version**\n    2.3 \n\n";
m["bbp::rtneuron::RTNeuron::waitRecord"] = "\nWait for the last frame of a movie recording to be issued. \n\nDo not call outside the main thread.\n\n**Version**\n    2.3 \n\n";
m["bbp::rtneuron::RecordingParams"] = "\nParameters to configure the generation of movies by ::`RTNeuron.record`. \n";
m["bbp::rtneuron::RecordingParams::cameraPath"] = "\nThe camera path to be used during rendering. If not assigned each view will keep its own camera manipulator. \n";
m["bbp::rtneuron::RecordingParams::cameraPathDelta"] = "\nDelta time in milliseconds which the camera path is advanced each frame. If 0 real time will be used. \n";
m["bbp::rtneuron::RecordingParams::fileFormat"] = "\nExtension (without dot) of the file format to use. File formats supported are those for which an OSG plugin is avaiable. \n";
m["bbp::rtneuron::RecordingParams::filePrefix"] = "\nPrefix to append to the output files. \n";
m["bbp::rtneuron::RecordingParams::frameCount"] = "\nIf different from 0, sets the number of frames to render before stop recording. **Version**\n    2.3 \n\n";
m["bbp::rtneuron::RecordingParams::simulationDelta"] = "\nDelta time milliseconds in which the simulation is advanced each frame. \n";
m["bbp::rtneuron::RecordingParams::simulationEnd"] = "\nEnd timestamp in milliseconds. \n";
m["bbp::rtneuron::RecordingParams::simulationStart"] = "\nStart timestamp in milliseconds. \n";
m["bbp::rtneuron::RecordingParams::stopAtCameraPathEnd"] = "\nSet to true to stop recording at the end of the camera path if cameraPathDelta is a positive number. The camera path time interval is considered open at the right. \n";
m["bbp::rtneuron::RepresentationMode"] = "\nRepresentation mode for neurons \n\n*Values:*\n * SOMA * SEGMENT_SKELETON * WHOLE_NEURON * NO_AXON * NO_DISPLAY * NUM_REPRESENTATION_MODES";
m["bbp::rtneuron::Scene"] = "\nThe scene to be rendered by one or more views.\n\nA scene contains the circuit elements to be displayed, additional mesh models and is associated with simulation data (this may be moved to the View class in the future).\n\nThe attributes that can be passed to ::`RTNeuron.createScene` are the following:\n *accurate_headlight* (bool): Apply shading assuming directional light rays from the camera position or parallel to the projection plane.\n\n *alpha_blending* (AttributeMap): If provided, transparent rendering will be enabled in this scene. The attributes to configure the alpha-blending algorithm are:\n   *mode* (string): depth_peeling, multilayer_depth_peeling or fragment_linked_list if compiled with OpenGL 3 support.\n\n   *max_passes* (string): Maximum number of rendering passes for multipass algorithms.\n\n   *cutoff_samples* (string): In multipass algorithms, the number of samples returned by the occlusion query at which the frame can be considered finished.\n\n   *slices* (int) [only for multi-layer depth peeling]: Number of slices to use in the per-pixel depth partition of the scene. If the input attribute map is empty, transparent rendering will be disabled.\n\n\n *circuit* (string): URI of the circuit to use for this scene.\n\n *connect_first_order_branches* (bool): Translate the start point of first order branches to connect them to the soma (detailed or spherical depending on the case).\n\n *em_shading* (bool): Choose between regular phong or fake electron microscopy shading.\n\n *inflatable_neurons* (bool): If true the neuron models can be inflated by displacing the membrame surface in the normal direction. The inflation factor is specified as a view attribute called *inflation_factor*.\n\n *load_morphologyes* (bool): Whether load morphologies for calculating soma radii or not.\n\n *lod* (AttributeMap): Level of detail options for different types of objects\n   *neurons* (AttributeMap): Each attribute is a pair of floats between [0, 1] indicating the relative range in which a particular level of detail is used. Attribute names refer to levels of detail and they can be: *mesh*, high_detail_cylinders*, *low_detail_cylinders*, *tubelets*, *detailed_soma*, spherical_soma*\n\n\n *mesh_based_partition* (bool): Use the meshes for load balancing spatial partitions. Otherwise only the morpholgies are used. This options requires use_meshes to be also true. \n  **Version**\n      2.4\n\n\n *partitioning* (DataBasePartitioning): The type of decomposition to use for DB (sort-last) partitions.\n\n *preload_skeletons* (bool): Preload all the capsule skeletons used for view frustum culling into the GPU instead of doing the first type they are visible.\n\n *unique_morphologies* (bool) If true, enables optimizations in spatial partitions that are only possible assuming that morphologies are unique. \n  **Version**\n      2.4\n\n\n *use_cuda* (bool): Enable CUDA based view frustums culling.\n\n *use_meshes* (bool): Whether triangular meshes should be used for neurons or not.\n\n\nScenes must be created using ::`RTNeuron.createScene` before the Equalizer configuration is started. At creation time scenes are assigned an internal ID. In multi-procress configurations (be it in the same machine or not), the scenes to used must be created in the same order to ensure the consistency of the frames.\n\nAt this moment, scene changes are not propagated from the application process to the rendering clients. \n";
m["bbp::rtneuron::Scene::Object::apply"] = "\nApplies an operation to this scene object.\n\nThe operation is applied immediately, there is no need to call ::`update()`. The operation is distributed to all the nodes participating in the Equalizer configuration.\n\n**Version**\n    2.7 \n\n**Exceptions**\n\n     ``std.runtime_error`` -       if the operation does not accept this object as input.\n\n";
m["bbp::rtneuron::Scene::Object::getObject"] = "\nReturn a copy of the object passed to the add method that returned this handler.\n\nThese objects will be: A read-only numpy array of u4 for neurons, with the neuron GIDs.\n\n A brain.Synapses container for add{A,E}fferentNeurons\n\n A string with the model name for addModel\n\n None for addGeometry \n\n";
m["bbp::rtneuron::Scene::Object::query"] = "\nReturn a handler to a subset of the entities from this object.\n\nThe function shall throw if any of the ids does not identify any entity handled by this object.\n\nAttribute changes on the subset handler will affect only the entities selected. Attribute changes on the parent handler will still affect all the entities. The child handler attributes are also updated when the parent attributes are modified. However, attribute updates on a child handler are not propagated to the attributes of any other children (regardless of having overlapping subsets). Nevertheless, when calling ::`update()` changes in the attributes are always made effective when needed.\n\nThe lifetime of the returned object is independent of the source one, but the returned object will be invalidated (operations will throw) when the source one is deallocated or invalidated. If this function is called recursively, the new returned objects will depend on the parent of the called object.\n\nSubset handlers are not part of the objects returned by ::`Scene.getObjects()`.\n\nBeware that attribute updates on subhandlers may be discarded if the parent object has not been fully integrated in the scene yet (i.e., no frame including it has been rendered).\n\nThis method may not be implemented by all objects.\n\n**Version**\n    2.6 \n\n";
m["bbp::rtneuron::Scene::Object::update"] = "\nIssues and update operation on the scene object managed by this handler.\n\nAttributes are copied internally so it's safe to modify the attributes after an update, however those changes won't take effect until update is called again. \n";
m["bbp::rtneuron::Scene::addAfferentSynapses"] = "\nAdds a set of synapse glyphs at their post-synaptic locations to the scene.\n\nThread safe with regard to the rendering loop.\n\n**Return**\n    An object handler to the synapse set added.\n\n**Version**\n    2.2 \n\n**Version**\n    2.4 For the *surface* attribute \n\n**Parameters**\n\n     ``synapses`` -       The synapse container. \n\n     ``attributes`` -       Synapse display attributes:\n       radius (float)\n\n       color (floatx4)\n\n       surface (bool): If true, the synapses are placed on the surfaces of the geometry, or in the center otherwise \n\n\n";
m["bbp::rtneuron::Scene::addEfferentSynapses"] = "\nAdds a set of synapse glyphs at their pre-synaptic locations to the scene.\n\nExactly the same as the function addAfferentSynapses but for efferent synapses.\n\n**Return**\n    An object handler to the synapse set added.\n\n**Version**\n    2.2 \n\n";
m["bbp::rtneuron::Scene::addGeometry"] = "\nAdds geometry described as vertices and faces to the scene.\n\n**Return**\n    An object handler to the model added.\n\n**Version**\n    2.12 \n\n**Parameters**\n\n     ``vertices`` -       A Nx4 numpy array of floats or a list of 4-element lists for adding points with size/radii to the scene. A Nx3 array or 3-element lists for all other cases (points with a single radius \n\n     ``primitive`` -       If None, the vertices will be added as points to the scene. For adding lines or triangles this parameter must be a MxI numpy array of integeres or N list of I-element lists, where I is 2 for lines and 3 for triangles. \n\n     ``colors`` -       An Nx4 optional numpy array or N lists of 4-element iterables for per vertex colors, or a single 4-element iterable for a global color. If not provided a default color will be used. \n\n     ``normals`` -       An optional Nx3 numpy array or a list of 3-element iterables with per vertex normals. Not used for points and spheres. \n\n     ``attributes`` -       Optional attributes concerning shading details\n       flat (bool): If true, the normal array is ignored and flat shading is used instead. If false and no normal array is provided, vertex normals are computed on the fly. Flat shading is only meaningful for triangle meshes.\n\n       line_width (float): Line width. Only for line primitives.\n\n       point_size (float): For points without individual size/radius this is the overall size. Its interpretation depends on the point style. For spheres, it's the radius. For points or circles, this is the screen size in pixels. If not specified, it will default to 1.\n\n       point_style (string): Use \"spheres\" to add real 3D spheres to the scene, \"points\" to add round points sprites and \"circles\" to add circles with 1 pixel of line width (the last two use regular GL_POINTS style). The default style if not specified is points.\n\n\n";
m["bbp::rtneuron::Scene::addModel(const char *, const Matrix4f&, const AttributeMap&)"] = "\nLoads a 3D model from a file and adds it to the scene.\n\n**Return**\n    An object handler to the model added.\n\n**Warning**\n    Additional models are not divided up in DB decompositions.\n\n**Version**\n    2.2 \n\n**Parameters**\n\n     ``filename`` -       Model file to load. \n\n     ``transform`` -       An affine transformation to apply to the model. \n\n     ``attributes`` -       Model attributes:\n       color (floatx4) The diffuse color to be applied to the model to all parts that don't specify any material already.\n\n       flat (bool): If true and the model doesn't include it's own shaders, a shader to render facets with flat shading will be applied. If false, it has no effect.\n\n\n";
m["bbp::rtneuron::Scene::addModel(const char *, const char *, const AttributeMap&)"] = "\nConvenience overload of the function above.\n\n**Return**\n    An object handler to the model added.\n\n**Warning**\n    Additional models are not divided up in DB decompositions\n\n**Version**\n    2.2 \n\n**Parameters**\n\n     ``filename`` -       Model file to load. \n\n     ``transform`` -       A sequence of affine transformations. The sequence is specified as a colon separated string of 3 possible transformations:\n       rotations \"r@x,y,z,angle\"\n\n       scalings \"s@x,y,z\"\n\n       translations \"t@x,y,z\" \n\n\n     ``attributes`` -       See function above.\n\n";
m["bbp::rtneuron::Scene::addNeurons"] = "\nAdd a set of neurons to the scene.\n\nThis is an asynchronous operation. The neuron container as well as the attribute map are copied internally so it is safe to modify them afterwards. Thread safe with regard to the rendering loop.\n\n**Return**\n    An object handler to the neuron set added. \n\n**Parameters**\n\n     ``gids`` -       The GIDs of the neurons to add.\n\n     ``attributes`` -       Neuron display attributes:\n       mode (RepresentationMode): How to display neurons. Neurons added with SOMA or NO_AXON modes cannot be switched to WHOLE_NEURON later on.\n\n       color_scheme (ColorScheme): Coloring method to use. SOLID_COLOR by default is not provided\n\n       color (floatx4): RGBA tuple to be used as base color for SOLID_COLOR and BY_WIDTH_COLORS color schemes.\n\n       colormaps (AttributeMap): Optional submap with target specific color maps. These color maps override the color maps from the view. The supported color maps are:\n         *by_distance_to_soma*: The color map to use for the BY_DISTANCE_TO_SOMA coloring scheme.\n\n         *by_width*: The color map to use for the BY_WIDTH coloring scheme.\n\n         *compartments*: The color map to use for compartmental simulation data.\n\n         *spikes*: The color map to use for spike rendering. The range of this color map must be always [0, 1], otherwise the rendering results are undefined.\n\n\n       primary_color (floatx4): An alias of the above.\n\n       secondary_color (floatx4): RGBA tuple to be used as secondary color for BY_WIDTH_COLORS.\n\n       max_visible_branch_order (int): Changes the maximum branching order of visible sections. Use -1 to make all braches visible and 0 to make only the soma visible.\n\n\n";
m["bbp::rtneuron::Scene::cellSelected"] = "\nSignal emitted when a cell is selected by pick. \n";
m["bbp::rtneuron::Scene::cellSetSelected"] = "\nSignal emitted when a group of cells is selected by pick.\n\n**Version**\n    2.7 \n\n";
m["bbp::rtneuron::Scene::clear"] = "\nRemoves all the objects from the scene.\n\nClipping planes are removed.\n\nTo be called only from the application node. \n";
m["bbp::rtneuron::Scene::clearClipPlanes"] = "";
m["bbp::rtneuron::Scene::clearSimulation"] = "\nClear any simulation report from the scene. \n";
m["bbp::rtneuron::Scene::getAttributes"] = "\nThe runtime configurable attribute map.\n\nThe modifiable attributes are: *alpha_blending* (AttributeMap): The attribute map with options for transparency algorithms. See Scene class documentation\n\n *em_shading* (bool): See Scene class documentation\n\n *auto_update* (bool): Whether scene modifications automatically trigger a dirty signal or not.\n\n *inflatable_neurons* (bool): Enable/disable neuron membrame inflation along the surface normal. The inflation factor is specified as a view attribute called *inflation_factor*. \n\n";
m["bbp::rtneuron::Scene::getCircuit"] = "\nGet the current brain.Circuit to be used for this scene \n";
m["bbp::rtneuron::Scene::getCircuitSceneBoundingSphere"] = "\n**Return**\n    A tuple with the scene center and radius. \n\n";
m["bbp::rtneuron::Scene::getClipPlane"] = "\nQueries the clip plane with a given index.\n\n**Version**\n    2.13 \n\n**Exceptions**\n\n     ``runtime_error`` -       if no plane has been assigned in that index. \n\n";
m["bbp::rtneuron::Scene::getHighlightedNeurons"] = "\nReturn the gids of the highlighted neurons.\n\n**Return**\n    A numpy array of u4 copied from the internal list. \n\n";
m["bbp::rtneuron::Scene::getNeuronSelectionMask"] = "\nGets the set of unselectable cells.\n\nThis mask affects the results of ::`pick()` functions.\n\n**Return**\n    A numpy array of u4 with the unslectable neurons. \n\n**Version**\n    2.9 \n\n";
m["bbp::rtneuron::Scene::getObjects"] = "\nReturns the handlers to all objects added to the scene.\n\n**Return**\n    A list of object handlers. \n\n";
m["bbp::rtneuron::Scene::getSomasBoundingSphere()"] = "\n**Return**\n    The center and radius around the somas of the scene. \n\n";
m["bbp::rtneuron::Scene::getSynapsesBoundingSphere()"] = "\n**Return**\n    The center and radius around the synapses of the scene. \n\n";
m["bbp::rtneuron::Scene::highlight(const GIDSet&, bool)"] = "\nToggle highlighing of a cell set\n\nTo be called only from the application node. **Parameters**\n\n     ``gids`` -       The set of cells to toggle. \n\n     ``on`` -       True to highlight the cell, false otherwise. \n\n";
m["bbp::rtneuron::Scene::pick(const Vector3f&, const Vector3f&) const"] = "\nIntersection test between the pointer ray and the scene elements.\n\nMay emit cellSelected or synapseSelected signals if a scene object was hit.\n\nA signal is used to communicate the result to allow decoupling the GUI event handling code from selection action callbacks.\n\n**Version**\n    2.7 \n\n**Parameters**\n\n     ``origin`` -       world space origin of the pick ray \n\n     ``direction`` -       pick ray direction, does not need to be normalized\n\n";
m["bbp::rtneuron::Scene::pick(const View&, float, float, float, float) const"] = "\nIntersection test of the space region selected by a rectangular area projected using the camera from the given view.\n\nThe implementation distinguishes between perspective and orthographic projections. Will emit a cellSetSelected signal with the group of somas intersected by the projection of the rectangle (both towards infinite and the camera position).\n\nA signal is used to communicate the result to allow decoupling the GUI event hanlding code from selection action callbacks.\n\n**Version**\n    2.7 \n\n**Parameters**\n\n     ``view`` -       \n\n     ``left`` -       Normalized position (in [0,1]) of the left side of the rectangle relative to the camera projection frustum/prism. \n\n     ``right`` -       Normalized position (in [0,1]) of the right side of the rectangle relative to the camera projection frustum/prism. \n\n     ``bottom`` -       Normalized position (in [0,1]) of the bottom side of the rectangle relative to the camera projection frustum/prism. \n\n     ``top`` -       Normalized position (in [0,1]) of the top side of the rectangle relative to the camera projection frustum/prism.\n\n";
m["bbp::rtneuron::Scene::progress"] = "\nEmitted as scene loading/creation advances. \n";
m["bbp::rtneuron::Scene::remove"] = "\nRemoves a target/model from the scene given its handler. \n";
m["bbp::rtneuron::Scene::setCircuit"] = "\nSet the brain.Circuit to be used for this scene. Throws if the scene already contains neurons or synapses. \n";
m["bbp::rtneuron::Scene::setClipPlane"] = "\nAdds or modifies a clipping plane.\n\nClipping planes are only applied to subscenes that have no spatial decomposition, otherwise they are silently ignored.\n\n**Version**\n    2.12 \n\n**Parameters**\n\n     ``index`` -       Number of plane to be set. The maximum number of clipping planes is 8. \n\n     ``plane`` -       Plane equation of the clipping plane. \n\n**Exceptions**\n\n     ``runtime_error`` -       if index is >= 8. \n\n";
m["bbp::rtneuron::Scene::setSimulation(const CompartmentReportPtr&)"] = "\nThread safe with regard to the rendering loop. \n";
m["bbp::rtneuron::Scene::setSimulation(const SpikeReportReaderPtr&)"] = "\nThread safe with regard to the rendering loop. \n";
m["bbp::rtneuron::Scene::synapseSelected"] = "\nSignal emitted when a synapse is selected by pick. Do not store the synapse argument passed to the callback.\n\n**Version**\n    2.10 \n\n";
m["bbp::rtneuron::Scene::update"] = "\nTo use when auto_update is false to trigger the scene update.\n\nIf the auto_update attribute is false, adding/removing objects from the scene or changing attributes that modify the rendering style will not trigger a new frame and consequent scene update. This function can be used to trigger it manually. \n";
m["bbp::rtneuron::SimulationPlayer"] = "\nInterface to simulation playback control. \n\nThe simulation timestamp if part of the frame data, this implies that all views are rendered with the same timestamp. \n";
m["bbp::rtneuron::SimulationPlayer::PlaybackState"] = "\nPlayback state for the simulation.\n\nState transitions are: PLAYING -> PAUSED if ::`SimulationPlayer.pause` is called. PLAYING -> FINISHED when one of the simulation window edges is reached.\n\nFINISHED -> PAUSED if ::`SimulationPlayer.pause` is called. FINISHED -> PLAYING if setSimulationTimestamp is called with a valid timestamp or setSimulationDelta is called.\n\nPAUSED -> PLAYING if ::`SimulationPlayer.play` is called.\n\n**Version**\n    2.7 \n\n\n*Values:*\n * PLAYING: \nState change emitted when ::`SimulationPlayer.play` is called and the previous state was paused or finished \n * PAUSED: \nState change emitted when ::`SimulationPlayer.pause` is called and the previous state was playing \n * FINISHED: \nState change emitted when playback reaches one edge of the playback window. The signal is emmited at the moment the timestamp is requested, but the current timestamp may be older. The signal timestampChanged should be used to know exactly the timestamp of the next frame to be displayed. \n";
m["bbp::rtneuron::SimulationPlayer::adjustWindow"] = "\nAdjusts the simulation playback window to the reports of the active scenes.\n\nThe begin time will be equal to the minimum of the start time of all reports and the end time will be equal to the maximum of end time of all reports.\n\nFor stream based reports, this function will try to update the end timestamp if the playback state is paused.\n\nThe timestamp will be clamped to the new window and a new frame will be triggered if necessary.\n\n**Version**\n    2.7 \n\n**Exceptions**\n\n     ``runtime_error`` -       if there's no active scene with a report attached.\n\n";
m["bbp::rtneuron::SimulationPlayer::finished"] = "";
m["bbp::rtneuron::SimulationPlayer::getTimestamp"] = "\nThe timestamp being displayed currently or NaN if undefined. \n";
m["bbp::rtneuron::SimulationPlayer::pause"] = "\nPause simulation playback. \n";
m["bbp::rtneuron::SimulationPlayer::play"] = "\nStart simulation playback from the current timestamp. \n";
m["bbp::rtneuron::SimulationPlayer::playbackStateChanged"] = "\nSignal emitted when simulation playback state is changed.\n\n**Version**\n    2.7 \n\n";
m["bbp::rtneuron::SimulationPlayer::setBeginTime"] = "";
m["bbp::rtneuron::SimulationPlayer::setEndTime"] = "";
m["bbp::rtneuron::SimulationPlayer::setSimulationDelta"] = "\nThe timestep between simulation frames to be used at playback. \n";
m["bbp::rtneuron::SimulationPlayer::setTimestamp"] = "\nSets the next timestamp to display and triggers rendering.\n\nFrom version 2.7 on, it may throw when trying to move the timestamp beyond the end of a stream-based report. \n";
m["bbp::rtneuron::SimulationPlayer::setWindow"] = "\nA (double, double) tuple with the simulation playback time window.\n\nIf written, the timestamp to display is clamped to the new window, a new frame is triggered if necessary and simulation window auto-adjustment is turned off (to turn it on again set RTNeuron.attributes.auto_adjust_simulation_window to True). \n";
m["bbp::rtneuron::SimulationPlayer::simulationDeltaChanged"] = "\nSignal emitted when simulation delta is changed.\n\n**Version**\n    2.7 \n\n";
m["bbp::rtneuron::SimulationPlayer::timestampChanged"] = "\nSignal emitted whenever a new frame with a new timestamp has finished. \n";
m["bbp::rtneuron::SimulationPlayer::windowChanged"] = "\nSignal emitted when simulation window is changed.\n\nThe signal is emitted be either calls to setWindow or by simulation window auto-adjustment (see ::`RTNeuron.RTNeuron()` for details).\n\n**Version**\n    2.7 \n\n";
m["bbp::rtneuron::TrackballManipulator"] = "\nDefault mouse-based manipulator from OpenSceneGraph. \n\nUse the right button to zoom, middle button to pan and left button to rotate. \n\nInherits from bbp.rtneuron.CameraManipulator\n";
m["bbp::rtneuron::TrackballManipulator::getHomePosition"] = "\n**Return**\n    A tuple (eye, center, up) where each one is an [x, y, z] vector \n\n**Version**\n    2.3 \n\n";
m["bbp::rtneuron::TrackballManipulator::setHomePosition"] = "\n**Version**\n    2.3 \n\n**Parameters**\n\n     ``eye`` -       The reference camera position \n\n     ``center`` -       The reference pivot point for rotations \n\n     ``up`` -       The direction of the up direction in the reference orientation. The \"look at\" vector is (center - eye). \n\n";
m["bbp::rtneuron::VRPNManipulator"] = "\nA camera manipulator which is controlled via a VRPN device. The current supported devices are space mouse and Wiimote. \n\nInherits from bbp.rtneuron.CameraManipulator\n";
m["bbp::rtneuron::VRPNManipulator::VRPNManipulator(const DeviceType, const AttributeMap&)"] = "\nCreate a VRPN manipulator using the given configuration\n\n\nFor SPACE_MOUSE and WIIMOTE *analog* (string|AttributeMap): If it is a string, it contains the VRPN device URL If it is an attribute map, the map contains:\n   *url* (string): The device specification for the device.\n\n\n *url* (string): The URL of the VRPN device. Provided for consistency with GYRATION_MOUSE and INTERSENSE_WAND and with forward compatibility in mind.\n\nIf neither *analog* nor *url* are given, the URL will default to \"WiiMote@localhost\" for WIIMOTE and \"device@localhost\" for SPACE_MOUSE. The same device will be used for the device button. \n**Parameters**\n\n     ``type`` -       The device type. \n\n     ``configuration`` -       An attribute map with the device-specific configuration. For GYRATION_MOUSE and INTERSENSE_WAND the configuration attributes are:\n       *tracker* (AttributeMap):\n         *url* (string): The VPRN device URL.\n\n         *sensorid* (int | \"any\" | \"lowest\"): The sensor ID for the tracker. Defaults to \"any\" if not provided.\n\n         *attitude_axis* (string or floatx9): A base change matrix from the device coordinates to world coordinates for the attitude axis. Defaults to the identity matrix is not given.\n\n         *position_axis* (string or floatx9): A base change matrix from the device coordinates to world coordinates for the tracker position. Defaults to the identity matrix is not given\n\n\n       *button* (string|AttributeMap): If it is a string, it contains the VRPN device URL If it is an attribute map, the map contains:\n         *url* (string): The device specification for the device.\n\n\n       *analog* (string|AttributeMap): If it is a string, it contains the VRPN device URL If it is an attribute map, the map contains:\n         *url* (string): The device specification for the device.\n\n\n       *url* The default URL for VRPN devices that are not specified explicitly or which do not have a *url* attribute.\n\n      Any of *tracker*, *analog* and *button* can be omitted if *url* is given. In that case, additional attributes take default values.\n\n";
m["bbp::rtneuron::View"] = "\nThis class represents a view on a scene.\n\nA view holds together a scene, a camera and the visual attributes that are not bound to a scene, e.g. level of detail bias, simulation color map, stereo correction. A view can also have a camera manipulator and a selection pointer. Cameras are view specific and cannot be shared.\n\nThere is a one to one mapping between RTNeuron views and Equalizer views.\n\n**Note**\n    Currently the simulation report is bound to the scene but this will be moved to the view in the future. The same applies to enabling/disabling alpha blending at runtime. \n\n";
m["bbp::rtneuron::View::computeHomePosition"] = "\nCompute the home position for the current scene and set it to the camera manipulator.\n\nThe camera position is also reset to new home position. **Version**\n    2.4 \n\n";
m["bbp::rtneuron::View::getAttributes"] = "\nAttribute map with runtime configurable attributes for a View.\n\nExisting attributes are: General:   *background* (floatx4): Background color. The alpha channel of the background is considered by frame grabbing functions. If alpha equals to 1, the output images will have no alpha channel.\n\n   *use_roi* (float): Compute and use regions of interest for frame readback in parallel rendering configurations.\n\n\n Appearance:\n   *clod_threshold* (float): When using continuous LOD, the unbiased distance at which the transition from pseudocylinders to tublets occurs for branches of radius 1. This value is modulated by the lod_bias. During rendering, the distance of a segment is divided by its radius before comparing it to the clod_threshold.\n\n   *colormaps* (AttributeMap): A map of ColorMap objects. The currently supported color maps are:\n     *compartments*: The color map to use for compartmental simulation data.\n\n     *spikes*: The color map to use for spike rendering. This range of this color map must be always [0, 1], otherwise the rendering results are undefined.\n\n\n   *display_simulation* (bool): Show/hide simulation data.\n\n   *idle_AA_steps* (int): Number of frames to accumulate in idle anti-aliasing\n\n   *highlight_color* (floatx4): The color applied to make highlighted neurons stand out. The highlight color replaces the base color when *display_simulation* is disabled. When *display_simulation* is enabled, the highlight color is added to the color obtained from simulation data mapping.\n\n   *inflation_factor* (float): Sets the offset in microns by which neuron membrane surfaces will be displaced along their normal direction. This parameter has effect only on those scenes whose *inflatable_neurons* attribute is set to true.\n\n   *lod_bias* (float): A number between 0 and 1 that specifies the bias in LOD selection. 0 goes for the lowest LOD and 1 for the highest.\n\n   *probe_color* (floatx4): The color to apply to those parts of a neuron whose simulation value is above the threshold if simulation display is enabled.\n\n   *probe_threshold* (float): The simulation value above which the probe color will be applied to neuron surfaces if simulation display is enabled.\n\n   *spike_tail* (float): Time in millisecond during which the visual representation of spikes will be still visible.\n\n\n Frame capture\n   *snapshot_at_idle* (bool): If true, take snapshots only when the rendering thread becomes idle (e.g. antialias accumulation done). Otherwise, the snapshot is taken at the very next frame.\n\n   *output_file_prefix* (string): Prefix for file written during recording.\n\n   *output_file_format* (string): File format extension (without dot) to use during frame recording. Supported extensions are those for which OSG can find a pluging.\n\n\n Cameras and stereo\n   *auto_compute_home_position* (bool): If true, the camera manipulator home position is recomputed automatically when the scene object is changed or when the scene emits its dirty signal.\n\n   *auto_adjust_model_scale* (bool): If true, every time the scene is changed the ratio between world and model scales is adjusted.\n\n   *depth_of_field* (AttributeMap): Attributes to enable and configure depth of field effect.\n     *enabled* (bool)\n\n     *focal_distance* (float): Distance to the camera in world units at which objects are in focus\n\n     *focal_range* (float): Distance from the focal point within objects remain in focus.\n\n\n   *model_scale* (bool) : Size hint used by Equalizer to setup orthographic projections and stereo projections. Set to 1 in order to use world coordinates in orthographic camera frustums.\n\n   *stereo_correction* (float): Multiplier of the scene size in relation to the observer for stereo adjustment.\n\n   *stereo* (bool) : Enables/disables stereoscopic rendering.\n\n   *zero_parallax_distance* (float): In stereo rendering, the distance from the camera in meters at which left and right eye projections converge into the same image (only meaningful for fixed position screens). All valid attributes are initialized to their default values. \n\n\n";
m["bbp::rtneuron::View::getCamera"] = "";
m["bbp::rtneuron::View::getCameraManipulator"] = "";
m["bbp::rtneuron::View::getColorMap"] = "";
m["bbp::rtneuron::View::getPointer"] = "";
m["bbp::rtneuron::View::getScene"] = "";
m["bbp::rtneuron::View::getSpikeColorMap"] = "\n**Version**\n    2.6 \n\n";
m["bbp::rtneuron::View::record"] = "\nEnable or disable frame grabbing.\n\n**See**\n    File naming attributes from View.attributes \n\n**Parameters**\n\n     ``enable`` -       If true, rendered images will be written to files starting from next frame on.\n\n";
m["bbp::rtneuron::View::setCameraManipulator"] = "\nSets the manipulator that controls de camera.\n\nThe camera manipulator will receive input events from this view and process them into a model matrix. At construction, a trackball manipulator is created by default.\n";
m["bbp::rtneuron::View::setColorMap"] = "\nSets the color map used to translate compartmental simulation data to colors.  \n";
m["bbp::rtneuron::View::setPointer"] = "";
m["bbp::rtneuron::View::setScene"] = "\nSets the scene to be displayed. \n";
m["bbp::rtneuron::View::setSpikeColorMap"] = "\nSets the color map used to render spikes.\n\nThe domain of the color map function must be [0, 1]. **Version**\n    2.6 \n\n";
m["bbp::rtneuron::View::snapshot(const std::string&, bool)"] = "\nTriggers a frame and writes the rendered image to a file.\n\nThis method waits until the image has been written unless waitForCompletion is false, in which case it returns inmediately.\n\nWhen idle AA is enabled and the *snapshot_at_idle* attribute is set, the snapshot is taken when frame accumulation is finished.\n\nThrows if fileName is empty.\n\n**Parameters**\n\n     ``fileName`` -       Filename including extension. If the filename include the squence \"%c\" all destination channels will be captured, replacing \"%c\" with the channel name in the output file. Notice that this option is meaningless for the offscreen snapshot functions. \n\n     ``waitForCompletion`` -       if true, locks until the image has been written to a file. \n\n";
m["bbp::rtneuron::View::snapshot(const std::string&, const tuple&)"] = "\nTriggers a frame on an auxiliary off-screen window and writes the rendered image to a file.\n\nThe off-screen window can have a different size than the windows in which this view resides. The vertical field of view of the camera will be preserved.\n\nThis method waits until the image has been written.\n\nWhen idle AA is enabled and the *snapshot_at_idle* attribute is set, the snapshot is taken when frame accumulation is finished.\n\nThrows if fileName is empty or if any of the resolution components is 0.\n\n**Parameters**\n\n     ``fileName`` -       Filename including extension. \n\n     ``resolution`` -       Tuple containing the horizontal and vertical resolution that will be used to generate the final image. \n\n";
m["bbp::rtneuron::View::snapshot(const std::string&, float)"] = "\nTriggers a frame on an auxiliary off-screen window and writes the rendered image to a file.\n\nThe off-screen window can have a different size than the windows in which this view resides. The vertical field of view of the camera will be preserved.\n\nThis method waits until the image has been written.\n\nWhen idle AA is enabled and the *snapshot_at_idle* attribute is set, the snapshot is taken when frame accumulation is finished.\n\nThrows if fileName is empty or scale is negative or zero.\n\n**Parameters**\n\n     ``fileName`` -       Filename including extension. \n\n     ``scale`` -       Scale factor that will be uniformly applied to the original view to obtain the final image. \n\n";
m["bbp::rtneuron::sceneops::NeuronClipping"] = "\nThis class provides a branch level clipping operation for neurons.\n\nCulling must be enabled in the scene that contains the target object. Otherwise this operation will have no effect.\n\nThe clipping state to apply is specified by a set of functions to make visible/invisible ranges of the morphological sections.\n\nThe culling mechanism discretizes sections in a predefined number of portions per section. Despite the API provides finer culling description, all the operations will work at the resolution defined by the implementation.\n\nThe current resolution is at most 32 portions per section (regardless of the section length).\n\nNeuron clipping is affected by the representation mode in the following ways: When all representation modes are available and the mode is changed, the clipping masks are cleared before applying the masks required by the new mode.\n\n If the neuron was created with NO_AXON or SOMA modes, changing the representation mode does not affect the current clipping.\n\n Clipping does not have any effect on the SOMA representation mode under any circumstances.\n\n When the representation mode is NO_AXON, axon sections cannot be unclipped.\n\n\nNeuron clipping respects spatial partitions of DB configurations.\n\n**Version**\n    2.7 \n\n\nInherits from bbp.rtneuron.Scene.ObjectOperation, boost.enable_shared_from_this< NeuronClipping >\n";
m["bbp::rtneuron::sceneops::NeuronClipping::clip"] = "\nMark section ranges for making them invisible.\n\nDiscrete section portions are clipped only if the given range fully contains them. The ranges are considered as closed interval. Ranges applied to section 0 (assumed to be the soma), are always converted into [0, 1]).\n\nSubsequent calls to NeuronClipping.unclip will cut/split/remove the ranges to be applied.\n\n**Return**\n    self for operation concatenation.\n\n**Parameters**\n\n     ``sections`` -       Section id list. Ids may be repeated. \n\n     ``starts`` -       Relative start positions of the ranges. Each value must be smaller than the value at the same position of the 'ends' vector, otherwise the range is ignored. \n\n     ``ends`` -       Relative end positions of the ranges. Each value must be greater than the value at the same position of the 'starts' vector, otherwise the range is ignored. \n\n**Exceptions**\n\n     ``std.invalid_argument`` -       if arrays have not the same size or if a range is ill-defined. \n\n";
m["bbp::rtneuron::sceneops::NeuronClipping::clipAll"] = "\nMake all neurites and optionally the soma invisible.\n\n**Return**\n    self for operation concatenation. \n\n**Parameters**\n\n     ``alsoSoma`` -       If true, the soma will be clipped. \n\n";
m["bbp::rtneuron::sceneops::NeuronClipping::unclipAll"] = "\nMake all neurites and the soma visible.\n\n**Return**\n    self for operation concatenation. \n\n";
}
